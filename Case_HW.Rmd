---
title: "STA380 Case Studies"
author: "Kevin Brill, Ananya Garg, Hannah Ho, Shane Kok"
date: "8/6/2019"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# STA380 Exercises

**Kevin Brill, Ananya Garg, Hannah Ho, Shane Kok**

Below are the results of our team's work on Dr. Scott's STA380 Predictive Modeling exercises.

## Visual Story Telling Part 1: Green Buildings

```{r GreenBuilding, message=FALSE, warning=FALSE}
library(mosaic)

green = read.csv('./data/greenbuildings.csv')
```
## Visual Story Telling Part 2: Flights at ABIA
```{r ABIA Departures, message=FALSE, warning=FALSE}
library(mosaic)
library(tidyverse)

month_arrival_avg=read.csv('./data/Month-Arrival-Avg.csv')
month_dep_avg=read.csv('./data/Month-Departure-Avg.csv')

months=c('1','2','3','4','5','6','7','8','9','10','11','12')

ggplot(data = month_dep_avg) + 
  geom_col(mapping = aes(x = Month, y = DepDelay)) + 
  facet_wrap(~ UniqueCarrier, nrow = 5)+
  labs(title = "Carrier Average Departure Delays by Month")+
  scale_x_discrete(name='Month',limits=months)

```

```{r ABIA Arrivals, message=FALSE, warning=FALSE}
ggplot(data = month_arrival_avg) + 
  geom_col(mapping = aes(x = Month, y = ArrDelay)) + 
  facet_wrap(~ UniqueCarrier, nrow = 5)+ 
  labs(title = "Carrier Average Arrival Delays by Month")+
  scale_x_discrete(name='Month',limits=months)

```

## Portfolio Modeling
```{r Portfolio, echo=FALSE, message=FALSE, warning=FALSE}
library(mosaic)
library(quantmod)
library(foreach)
set.seed(100)
#BALANCED PORTFOLIO
#-------------------------------------------------------------------------------
#### Now use a bootstrap approach
#### With more stocks

mystocks = c("USO", "VBK", "XLV", "VNQ", "VFH")
myprices = getSymbols(mystocks, from = "2017-01-01")


# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")") #for loop to adjust the OHLC for each ticker and store into a
	eval(parse(text=expr))
}

head(USOa)

# Combine all the returns in a matrix
all_returns = cbind(	ClCl(USOa),
								ClCl(VBKa),
								ClCl(XLVa),
								ClCl(VNQa),
								ClCl(VFHa))
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))

# Compute the returns from the closing prices
pairs(all_returns)

# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.05,0.3,0.05, 0.3, 0.3)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute your new total wealth
total_wealth = sum(holdings)


# Now loop over two trading weeks
total_wealth = 100000
weights = c(0.05,0.3,0.05, 0.3, 0.3)
holdings = weights * total_wealth
n_days = 20
wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
	return.today = resample(all_returns, 1, orig.ids=FALSE)
	holdings = holdings + holdings*return.today
	total_wealth = sum(holdings)
	wealthtracker[today] = total_wealth
}
total_wealth
plot(wealthtracker, type='l')


# Now simulate many different possible scenarios  
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.05,0.3,0.05, 0.3, 0.3)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

head(sim1)
hist(sim1[,n_days], 25)

# Profit/loss
mean(sim1[,n_days])
hist(sim1[,n_days]- initial_wealth, breaks=30)
```

VaR stands for value at risk, and this metric shows the risk of loss for an investment on a distribution. We calculated the VaR distribution for three different types of portfolios were tested for our portfolio modeling: a balanced, an aggressive, and a conservative portfolio. 

The aggressive portfolio is composed of small cap growth stock ETF’s, which means that it is focused on smaller stocks with potential. This is more risky because small cap companies have a larger chance of going under and are less stable than their large cap stock counterparts. As expected, the 5% VaR for the aggressive portfolio is the worst and is -$8000.

The conservative portfolio is mainly made up of different types of bond and money market ETF’s, which are one of the safest types of investments you can make on the financial market. Many of these are either mortgage or government backed bonds, therefore they are collateralized and this limits your risk significantly. The 5% VaR for the conservative portfolio is the best and is only -$1000.

The balanced portfolio contains a wide range of financial products from across the spectrum. These products contain ETF’s from a wide range of industries, such as oil and real estate. This spectrum of different industries helps to balance out the portfolio, and mostly Vanguard products were chosen for their strong returns and good past performance, in order to help balance out the risk of this portfolio. A healthcare ETF is super stable as well, which is why one of these ETF’s was included in the portfolio too. The 5% VaR for the balanced portfolio is about -$5000. 

The VaR distributions for each portfolio came out as expected, with the conservative portfolio being the least risky and the aggressive portfolio being the most. 

## Market Segmentation
```{r Market Segmentation, message=FALSE, warning=FALSE}

```
## Author Attribution
```{r getDTMTrain, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(tm)
library(tidyverse)
setwd("~/Grad School/Summer/Predictive Modeling/Second_Half/Homework/STA380_Case_Studies")
# Remember to source in the "reader" wrapper function
# it's stored as a Github gist at:
# https://gist.github.com/jgscott/28d9d1287a0c3c1477e2113f6758d5ff
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
## Rolling two directories together into a single corpus
author_dirs_train = Sys.glob('./data/ReutersC50/C50train/*')
file_list_train = NULL
labels_train = NULL
for(author in author_dirs_train) {
	author_name_train = substring(author, first=28)
	files_to_add_train = Sys.glob(paste0(author, '/*.txt'))
	file_list_train = append(file_list_train, files_to_add_train)
	labels_train = append(labels_train, rep(author_name_train, length(files_to_add_train)))
}

# Need a more clever regex to get better names here
all_docs_train = lapply(file_list_train, readerPlain) 
names(all_docs_train) = file_list_train
names(all_docs_train) = sub('.txt', '', names(all_docs_train))

mynames_train = file_list_train %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

documents_raw_train = Corpus(VectorSource(all_docs_train))
# Preprocessing
my_corpus_train = documents_raw_train
my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART"))

DTM_train = DocumentTermMatrix(my_corpus_train)

class(DTM_train)  # a special kind of sparse matrix format

DTM_train = removeSparseTerms(DTM_train, 0.975)
tfidf_mat_train = weightTfIdf(DTM_train)
```
```{r getDTMTest, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
author_dirs_test = Sys.glob('./data/ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL
for(author in author_dirs_test) {
  author_name = substring(author, first=27) #author name starts at 29th char
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs_test = lapply(file_list_test, readerPlain) 
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

# Clean up the file names, append author to filename
# This uses the piping operator from magrittr
# See https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html
mynames_test = file_list_test %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_docs_test) = mynames_test 
my_corpus_test = Corpus(VectorSource(all_docs_test))

# Preprocessing
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))

# A suboptimal but practical solution: IGNORE words you haven't seen before
# can do this by pre-specifying a dictionary in the construction of a DTM
DTM_test = DocumentTermMatrix(my_corpus_test,control = list(dictionary=Terms(DTM_train)))

tfidf_mat_test = weightTfIdf(DTM_test)
```
```{r Prediction Setup, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(class)
train_X = as.matrix(tfidf_mat_train)
train_y = labels_train
test_X = as.matrix(tfidf_mat_test)
test_y = labels_test
```
```{r PCA, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
train_X_centered = scale(train_X, center=TRUE, scale=FALSE)
test_X_centered = scale(test_X, center = TRUE, scale=FALSE)

#scrub_cols = which(colSums(X) == 0)
#X = X[,-scrub_cols]

pca_train = prcomp(train_X_centered)
pca_test <- predict(pca_train, newdata = test_X_centered)
pca_test <- as.data.frame(pca_test)


```
```{r KNN, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
cosine_dist_train = proxy::dist(train_X, method='cosine')
cosine_dist_test = proxy::dist(test_X,method='cosine')

knn.pred=knn(cosine_dist_train,cosine_dist_test,train_y,k=5) 
tab = table(knn.pred,test_y)

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
#55.24%
```
```{r KNN on PCA, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
pca_cosine_dist_train = proxy::dist(pca_train$x[,1:600], method='cosine')
pca_cosine_dist_test = proxy::dist(pca_test[,1:600],method='cosine')

knn.pred=knn(pca_cosine_dist_train,pca_cosine_dist_test,train_y,k=7) 
tab = table(knn.pred,test_y)

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
```
```{r Random Forest, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(randomForest)
set.seed(1)
#factorize response labels
train_y_fact = factor(train_y) 
test_y_fact = factor(test_y) 
#fit random forest
classifier <- randomForest(x = train_X, 
                           y = train_y_fact,
                           nTree = 10)

rand_forest_pred <- predict(classifier, newdata = test_X)
tab_rf = table(rand_forest_pred,test_y)

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab_rf)
```
```{r Random Forest with PCA, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(randomForest)
set.seed(1)
#factorize response labels
train_y_fact = factor(train_y) 
test_y_fact = factor(test_y) 
#fit random forest
classifier2 <- randomForest(x = pca_train$x[,1:600], 
                           y = train_y_fact,
                           nTree = 20)

rand_forest_pca_pred <- predict(classifier2, newdata = pca_test[,1:600])
tab_rf = table(rand_forest_pca_pred,test_y)

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab_rf)
```
```{r Random Forest with Parameter Tuning, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
train_y_fact = factor(train_y) 
test_y_fact = factor(test_y) 
#sqrt(1411) = 37
rf=randomForest(x=train_X,y=train_y_fact, mtry=37,importance=TRUE)

rand_forest_pred2 <- predict(rf, newdata = test_X)
tab_rf2 = table(rand_forest_pred2,test_y)

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab_rf2)
```


Our analysis of the author text data consisted of the following framework:

**1. Preprocessing of the training set**

**2. Preprocessing of the test set**

**3. PCA setup to allow for dimension reduction during modeling**

**4. Modeling and results**

**5. Further considerations**

Below, we dive into our processes for each of the above items.


##### 1. PREPROCESSING OF THE TRAINING SET

This section contains the bulk of the work done for this problem. We began by reading in the raw .txt files and extracting a list of all documents and a list of all author names in the training set. We then used the list of .txt files to construct the corpus for our problem. From this corpus, we convert all letters into lower case, remov all numbers; punctuation; and whitespaces, and delete all occurances of words that are contained in R's "SMART" stopwords dictionary. With the corpus fully cleaned, we create a DTM out of the corpus and remove all words that appear in only 2.5% of documents or fewer. In doing so, we guarantee that these rare words won't be given excessive weight in our models and skew our results. Finally, we replace the contents of our DTM with the TF-IDF weights of each word rather than simply their counts.

##### 2. PREPROCESSING OF THE TEST SET

In this section, we follow the same processes of the above section on the test set of documents, all the way up until we create the DTM. Here, we limit the dictionary of the test set's DTM to the words contained in the training set's DTM dictionary. In doing so, we ensure that both the training and testing sets' DTMs consist of the same set of columns when we begin modeling. Finally, we replace the contents of the test DTM with the TF-IDF weights of each word (same as above section).

##### 3. PCA SETUP TO ALLOW FOR DIMENSION REDUCTION DURING MODELING

Here, we find the PCAs of the training and testing sets. We find that the first 600 principal components explain ~80% of the variance in the training set so we proceed with the first 600 principal components.

##### 4. MODELING AND RESULTS

We obtain results for variations of two types of models: KNN and Random Forest. The prediction accuracies for each variant are listed below:

**KNN with cosine distance:** 55.24%

**KNN with cosine distance of first 600 principal components:** 58.08%

**Random Forest:** 60.48%

**Random Forest on first 600 principal components:** 55.28%

**Random Forest with m=sqrt(p):** 62.36%

As can be seen from the results above, our random forest model with m=sqrt(p)=37 and no dimensionality reduction yielded the best classification accuracy at 62.36% of articles attributed to the correct author.

##### 5. FURTHER CONSIDERATIONS

Our team tried to use Naive Bayes and BART to classify the articles, but ran into errors that took too long to debug before the deadline of this assignment. It is likely that these modeling techniques would outperform our above models.

Further, the best way to improve our classification accuracies would be to have more robust preprocessing. For example, we could create a dummy "unknown" variable in our test DTM that would count the number of words that show up in our test set that didn't appear in our training set. Our above test DTM simply ignore new words in the test set, which likely decreases performance. We could also consider n-gram groupings of words for more nuanced TF-IDF values in our DTMs.

## Association Rule Mining
```{r Association Rule Mining, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
library(igraph)
setwd("~/Grad School/Summer/Predictive Modeling/Second_Half/Homework/STA380_Case_Studies")
#read .txt file in basket format
groceries = read.transactions("./data/groceries.txt", format="basket", sep=",")

## Cast this variable as a special arules "transactions" class.
transactions = as(groceries, "transactions")

#run the apriori algorithm
apriori_results = apriori(transactions, parameter=list(support=.01, confidence=.5, maxlen=3))

#view results
inspect(subset(apriori_results, subset=lift > 2))
```

Write-up here
