---
title: "STA380 Case Studies"
author: "Kevin Brill, Ananya Garg, Hannah Ho, Shane Kok"
date: "8/6/2019"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# STA380 Exercises

**Kevin Brill, Ananya Garg, Hannah Ho, Shane Kok**

Maybe a little intro here before we get into the code.

## Visual Story Telling Part 1: Green Buildings

```{r GreenBuilding, message=FALSE, warning=FALSE}
library(mosaic)

green = read.csv('./data/greenbuildings.csv')
```
## Visual Story Telling Part 2: Flights at ABIA
```{r ABIA Departures, message=FALSE, warning=FALSE}
library(mosaic)
library(tidyverse)

month_arrival_avg=read.csv('./data/Month-Arrival-Avg.csv')
month_dep_avg=read.csv('./data/Month-Departure-Avg.csv')

months=c('1','2','3','4','5','6','7','8','9','10','11','12')

ggplot(data = month_dep_avg) + 
  geom_col(mapping = aes(x = Month, y = DepDelay)) + 
  facet_wrap(~ UniqueCarrier, nrow = 5)+
  labs(title = "Carrier Average Departure Delays by Month")+
  scale_x_discrete(name='Month',limits=months)

```

```{r ABIA Arrivals, message=FALSE, warning=FALSE}
ggplot(data = month_arrival_avg) + 
  geom_col(mapping = aes(x = Month, y = ArrDelay)) + 
  facet_wrap(~ UniqueCarrier, nrow = 5)+ 
  labs(title = "Carrier Average Arrival Delays by Month")+
  scale_x_discrete(name='Month',limits=months)

```

## Portfolio Modeling
```{r Portfolio, message=FALSE, warning=FALSE}

```
## Market Segmentation
```{r Market Segmentation, message=FALSE, warning=FALSE}

```
## Author Attribution
```{r getDTMTrain, message=FALSE, warning=FALSE}
library(tm)
library(tidyverse)
setwd("~/Grad School/Summer/Predictive Modeling/Second_Half/Homework/STA380_Case_Studies")
# Remember to source in the "reader" wrapper function
# it's stored as a Github gist at:
# https://gist.github.com/jgscott/28d9d1287a0c3c1477e2113f6758d5ff
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
## Rolling two directories together into a single corpus
author_dirs_train = Sys.glob('./data/ReutersC50/C50train/*')
file_list_train = NULL
labels_train = NULL
for(author in author_dirs_train) {
	author_name_train = substring(author, first=28)
	files_to_add_train = Sys.glob(paste0(author, '/*.txt'))
	file_list_train = append(file_list_train, files_to_add_train)
	labels_train = append(labels_train, rep(author_name_train, length(files_to_add_train)))
}

# Need a more clever regex to get better names here
all_docs_train = lapply(file_list_train, readerPlain) 
names(all_docs_train) = file_list_train
names(all_docs_train) = sub('.txt', '', names(all_docs_train))

mynames_train = file_list_train %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

documents_raw_train = Corpus(VectorSource(all_docs_train))
# Preprocessing
my_corpus_train = documents_raw_train
my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART"))

DTM_train = DocumentTermMatrix(my_corpus_train)

class(DTM_train)  # a special kind of sparse matrix format

## You can inspect its entries...
DTM_train = removeSparseTerms(DTM_train, 0.975)
tfidf_mat_train = weightTfIdf(DTM_train)
```

```{r getDTMTest}
author_dirs_test = Sys.glob('../data/ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL
for(author in author_dirs_test) {
  author_name = substring(author, first=28) #author name starts at 29th char
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs_test = lapply(file_list_test, readerPlain) 
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

# Clean up the file names, append author to filename
# This uses the piping operator from magrittr
# See https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html
mynames_test = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_docs_test) = mynames_test 
my_corpus_test = Corpus(VectorSource(all_docs_test))

# Preprocessing
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))

DTM_test = DocumentTermMatrix(my_corpus_test)
DTM_test # some basic summary statistics

# A suboptimal but practical solution: IGNORE words you haven't seen before
# can do this by pre-specifying a dictionary in the construction of a DTM
DTM_test = DocumentTermMatrix(my_corpus_test,
                        control =list(dictionary=Terms(DTM)))
```
```{r}
library(class)
train_X = as.matrix(tfidf_mat_train)
train_y = labels_train
```
```{r}

```
## Association Rule Mining
```{r Association Rule Mining, message=FALSE, warning=FALSE}

```

